{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# for building DQN model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTDQNAgent():\n",
    "    def __init__(self, env):        \n",
    "        self.state_size = -1 #env.getNumberOfPoints()**2\n",
    "        self.action_size = -1 #12\n",
    "        self.env = env\n",
    "        # hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.0001        \n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = 0.00006\n",
    "        self.epsilon_min = 0.00000001\n",
    "        \n",
    "        self.batch_size = 32    \n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)             \n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory.clear()  \n",
    "        \n",
    "    def initializeModel(self, env):\n",
    "        self.state_size = env.getNumberOfPoints()**2\n",
    "        self.action_size = 13\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def initializeModelForValidation(self, env, modelWeightsFileName):\n",
    "        self.state_size = env.getNumberOfPoints()**2\n",
    "        self.action_size = 13\n",
    "        self.model = self.build_model()\n",
    "        self.model.load_weights(modelWeightsFileName)\n",
    "        \n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2084, input_dim = self.state_size + self.action_size, activation ='relu'))\n",
    "        model.add(Dense(1024,activation ='relu'))\n",
    "        model.add(Dense(512,activation ='relu'))\n",
    "        model.add(Dense(256,activation ='relu'))\n",
    "        model.add(Dense(128,activation ='relu'))\n",
    "        model.add(Dense(1, activation ='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "    \n",
    "    # save sample <s,a,r,s'> to the replay memory \n",
    "    def append_sample(self, state, action, reward, next_state, terminal_state):\n",
    "        self.memory.append((state, action, reward, next_state, terminal_state))\n",
    "    \n",
    "    def prediction(self, state, poss_actions):\n",
    "        X_test = np.zeros((len(poss_actions), self.state_size + self.action_size))\n",
    "        for i in range(len(poss_actions)):\n",
    "            #print('poss_action: {}', poss_actions[i])\n",
    "            dummy = self.env.getStateActionEncoding(state, poss_actions[i])\n",
    "            X_test[i,:] = dummy\n",
    "        prediction = self.model.predict(X_test)\n",
    "        prediction = prediction.reshape(len(poss_actions))\n",
    "        return prediction\n",
    "    \n",
    "    def get_action(self, state, edge, episode):\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment\n",
    "        poss_actions = self.env.getPossibleActions(edge)\n",
    "        epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay*episode)\n",
    "            \n",
    "        if not poss_actions:\n",
    "            return [], epsilon\n",
    "        \n",
    "        if np.random.rand() <= epsilon: # Exploration: randomly choosing and action      \n",
    "            ptChosenForAction = np.random.choice(range(len(poss_actions)))\n",
    "            action = poss_actions[ptChosenForAction]\n",
    "        else: #Exploitation: this gets the action corresponding to max q-value of current state\n",
    "            q_values = self.prediction(state, poss_actions)\n",
    "            action_index = np.argmax(q_values)\n",
    "            action = poss_actions[action_index]            \n",
    "        return action, epsilon\n",
    "    \n",
    "    def get_action_for_validation(self, state, edge):\n",
    "        poss_actions = self.env.getPossibleActions(edge)\n",
    "        if not poss_actions:\n",
    "            return []\n",
    "        \n",
    "        q_values = self.prediction(state, poss_actions)\n",
    "        action_index = np.argmax(q_values)\n",
    "        action = poss_actions[action_index]            \n",
    "        return action\n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from the memory\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        update_input = np.zeros((self.batch_size, self.state_size + self.action_size))\n",
    "        #update_output = np.zeros((self.batch_size, self.action_size))\n",
    "        update_output = {}\n",
    "        actions, rewards, terminal_states = [], [], []        \n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            state, action, reward, next_state, terminal_state = mini_batch[i]\n",
    "            \n",
    "            newStateActions = self.env.getPossibleActionsForNewState(state, next_state)            \n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            terminal_states.append(terminal_state)            \n",
    "\n",
    "            update_input[i] = self.env.getStateActionEncoding(state, action)          \n",
    "            #print('num actions new state: {}', newStateActions)\n",
    "            #print('next state: {}'.format(next_state))\n",
    "            \n",
    "            if len(newStateActions) == 0:\n",
    "                update_output[i] = [0]\n",
    "            else:\n",
    "                update_output[i]= self.prediction(next_state, newStateActions)\n",
    "        \n",
    "        target = np.zeros((self.batch_size))\n",
    "            \n",
    "        # get your target Q-value on the basis of terminal state\n",
    "        for i in range(len(terminal_states)):\n",
    "            if terminal_states[i]:\n",
    "                target[i] = rewards[i]\n",
    "            else:\n",
    "                target[i] = rewards[i] + self.discount_factor * (np.amax(update_output[i]))\n",
    "\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "                    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
