{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CommonDefs import Point, Edge \n",
    "from TwoDimConvexHull import TwoDimConvexHull, PrintTwoDimConvexHull\n",
    "from Utils import nearestKNeighboursOfEdgeMidPt, checkTriangleForDelaunayCriteria\n",
    "from Graph import Graph\n",
    "from DTEnv import DTEnv\n",
    "from DTDQNAgent import DTDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\Thesis_Experiments\\Data\\DT_5_sorted.txt\", sep=\" \", header=None)\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointData = {}\n",
    "for i in range(len(df)):\n",
    "    pointId = 1\n",
    "    points = []\n",
    "    for j in range(0 , len(df.columns), 2):\n",
    "        if df.loc[i, j] == \"output\":\n",
    "            dtStartIdx = j + 1\n",
    "            break\n",
    "        else:\n",
    "            points.append(Point(pointId, df.loc[i, j], df.loc[i, j + 1]))\n",
    "            pointId = pointId + 1\n",
    "    pointData[i] = points\n",
    "\n",
    "#for key, value in pointData.items():\n",
    "    #print('key: {}, value: {}'.format(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the environment\n",
    "env = DTEnv()\n",
    "\n",
    "agent = DTDQNAgent(env)\n",
    "\n",
    "# tracking average reward per episode = total rewards in an episode/ total steps in an episode\n",
    "avg_reward = []\n",
    "\n",
    "# tracking total rewards per episode\n",
    "total_reward  = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Episodes = 2000\n",
    "for episode in range(0, Episodes):\n",
    "    \n",
    "    # tracking total rewards, step count\n",
    "    tot_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    #Reset the environment/Clear the previous states\n",
    "    env.reset()\n",
    "    env.initialize(pointData[0])\n",
    "    \n",
    "    # agent needs to be initialised only once since the DQN\n",
    "    # network will be initialised along with the agent\n",
    "    if episode == 0:\n",
    "        agent = DTDQNAgent(env)\n",
    "    \n",
    "    _, state = env.getStartState()\n",
    "    env.drawGraph()\n",
    "    terminal_state = False\n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        #Get the free edge from the list\n",
    "        edgeToProcess = env.getEdgesToProcess()[0]\n",
    "\n",
    "        action, epsilon = agent.get_action(state, edgeToProcess, episode)\n",
    "        reward = env.getReward(edgeToProcess, action)        \n",
    "        next_state = env.getNextState(edgeToProcess, action)\n",
    "        env.removeProcessedEdge(edgeToProcess)\n",
    "        terminal_state = env.isTerminalState()\n",
    "        env.drawGraph()\n",
    "        \n",
    "        \n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, terminal_state)\n",
    "        \n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        tot_reward += reward\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "        if terminal_state:\n",
    "            print('generated triangles: {}'.format(env.getGeneratedTriangles()))\n",
    "        \n",
    "        # Store the rewards\n",
    "        if terminal_state and episode % 4 ==0:\n",
    "            avg_reward.append(tot_reward/step_count)\n",
    "            total_reward.append(tot_reward)\n",
    "            print(\"episode:\", episode, \"  score:\", tot_reward, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", epsilon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_track = collections.defaultdict(dict)\n",
    "def initialise_tracking_states(state, action):\n",
    "    states_track[tuple(state)][tuple(action)] = []    #this is an array which will have appended values of that state-action pair for every 2000th episode   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will append latest Q-values of the 6 Q-values which are being tracked for checking convergence\n",
    "def save_tracking_states(agent):\n",
    "    for state in states_track.keys():\n",
    "        for action in states_track[state].keys():\n",
    "            Q = agent.prediction(state, [action])\n",
    "            states_track[state][action].extend(Q)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the object as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializeModel = True\n",
    "\n",
    "\n",
    "numData = int(df.shape[0] / 2)\n",
    "\n",
    "episodeStart = 0\n",
    "numEpisodes = df.shape[0] * 150\n",
    "currentEpisode = episodeStart;\n",
    "\n",
    "for i in range(0, numData):     \n",
    "    num_states_tracked = 0\n",
    "    \n",
    "    # reset epsilon start value and memory for each new configuration but keep the model parameters\n",
    "    # learned from the previous configuration\n",
    "    agent.reset()\n",
    "    \n",
    "    for episode in range(currentEpisode, numEpisodes):\n",
    "\n",
    "        # tracking total rewards, step count\n",
    "        tot_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        #Reset the environment/Clear the previous states\n",
    "        env.reset()\n",
    "        env.initialize(pointData[i])  \n",
    "        print('current point data index {} : episode : {}'.format(i, episode))\n",
    "        \n",
    "        if initializeModel:\n",
    "            agent.initializeModel(env)\n",
    "            initializeModel = False\n",
    "\n",
    "        _, state = env.getStartState()\n",
    "        #env.drawGraph()\n",
    "        terminal_state = False\n",
    "\n",
    "        while not terminal_state:\n",
    "\n",
    "            #Get the free edge from the list\n",
    "            edgeToProcess = env.getEdgesToProcess()[0]\n",
    "                        \n",
    "            action, epsilon = agent.get_action(state, edgeToProcess, episode)            \n",
    "            \n",
    "            '''if not action:\n",
    "                break'''\n",
    "            \n",
    "            #print('edgeToProcess: {}-{}'.format(edgeToProcess[0], edgeToProcess[1]))\n",
    "            #print('action chosen: {}'.format(env.getNodeIdFromPosAttr( action[4], action[5])))\n",
    "            \n",
    "            reward = env.getReward(edgeToProcess, action)        \n",
    "            next_state = env.getNextState(edgeToProcess, action)\n",
    "            env.removeProcessedEdge(edgeToProcess)\n",
    "            terminal_state = env.isTerminalState()\n",
    "            \n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, terminal_state)\n",
    "\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            tot_reward += reward\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            if terminal_state:\n",
    "                print('generated triangles: {}'.format(env.getGeneratedTriangles()))\n",
    "                \n",
    "            if reward > 0 and num_states_tracked < 2:\n",
    "                initialise_tracking_states(state, action)\n",
    "                save_tracking_states(agent)\n",
    "                num_states_tracked += 1\n",
    "\n",
    "            # Store the rewards\n",
    "            if terminal_state and episode % 10 ==0:\n",
    "                avg_reward.append(tot_reward/step_count)\n",
    "                total_reward.append(tot_reward)\n",
    "                print(\"episode:\", episode, \"  score:\", tot_reward, \"  memory length:\",\n",
    "                          len(agent.memory), \"  epsilon:\", epsilon)\n",
    "                \n",
    "        if episode % 1000 == 0:   \n",
    "            save_tracking_states(agent)  \n",
    "            \n",
    "        if episode % 1000 == 0:\n",
    "            agent.save(\"./Delaunay.h5\")\n",
    "        \n",
    "        if episode % 1000 == 0:\n",
    "            save_obj(avg_reward,'Rewards')   \n",
    "            save_obj(states_track,'States_tracked')\n",
    "    \n",
    "        if episode % 150 ==0 and episode !=0:\n",
    "            plt.plot(list(range(len(avg_reward))), avg_reward)\n",
    "            plt.show()\n",
    "            \n",
    "        if (episode % 150 == 0 and episode !=0):\n",
    "            currentEpisode = episode + 1\n",
    "            print('current episode: {}'.format(currentEpisode))            \n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "time = np.arange(0,60000)\n",
    "epsilon = []\n",
    "for i in range(0,60000):\n",
    "    epsilon.append(.00000001 + (1 - 0.00000001) * np.exp(-0.00006*i))\n",
    "                   \n",
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(states_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states_track)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "\n",
    "\n",
    "for key, value in states_track.items():\n",
    "    xaxis = np.asarray(range(500))\n",
    "    plt.plot(xaxis,np.asarray(states_track[key])[-500:])\n",
    "    plt.ylabel(\"Q-value\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
